<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="keywords" content="Yan Huang, 黄岩, CRIPAC, NLPR, CASIA, Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, UESTC" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="style.css" type="text/css" />
<!-- <link rel="shortcut icon" href="fig/cripac.png"> -->
<title>Yan Huang's Homepage</title>
</head>
<body>
<div id="layout-content">

<script type="text/javascript">
<!--
// Toggle Display of BibTeX
function toggleBibtex(articleid) {
  var bib = document.getElementById(articleid);
  // Toggle 
    if(bib.style.display == "none") {
      bib.style.display = "";
    }
    else {
      bib.style.display = "none";
    }
}
-->
</script>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-40926388-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<table class="imgtable"><tr><td>
<img src="fig/yhuang.jpg" alt="alt text" width="400px" height="210px" /> &nbsp;</td>
<td align="left">

<div id="toptitle"> 
  <h1>
  <a href="http://yanrockhuang.github.io./">Yan Huang</a> &nbsp; 黄岩
  </h1>
</div>

<p>
Associated Professor, <a href="https://scholar.google.com/citations?user=6nUJrQ0AAAAJ&hl=zh-CN">Google Scholar</a>
<br />
<br />
Email: <a href="mailto:yhuang@nlpr.ia.ac.cn">yhuang at nlpr.ia.ac.cn</a><br />
Address: No.95 ZhongGuanCun East St, HaiDian District, Beijing, China, 100190<br />
</p>
</td></tr></table>

<h2>News</h2>
<ul>
<li> <p>We are organizing an ICCV'19 Workshop of "CroMoL: Cross-Modal Learning in Real World", please visit https://cromol.github.io/ for details. </p></li> 
<li> <p>2019.5.19, One paper is accepted by PR ! </p></li>
<li> <p>2019.2.25, Two papers are accepted by CVPR 2019 ! One is oral presentation. </p></li>
</ul>

<h2>
  Biography 
</h2>
<p>
    I received the BSc degree from University of Electronic Science and Technology of China (UESTC) in 2012, 
  and the PhD degree from University of Chinese Academy of Sciences (UCAS) in 2017. Since July 2017, 
  I has joined the National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA) as an associate professor.
My research interests include computer vision and multimodal learning, especially cross-modal retrieval and video person analysis.
</p>


<h2>Journal Papers</h2> 
<ul>
<li><p>Weining Wang, <strong>Yan Huang</strong>, and Liang Wang, Long Video Question Answering: A Matching-guided Attention Model, <strong>PR</strong>, accepted, 2020. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320320300546" target="_self">PDF</a> </p></li>
<li><p>Kai Niu, <strong>Yan Huang</strong>, Wanli Ouyang, and Liang Wang, Improving Description-based Person Re-identification by Multi-granularity Image-text Alignments, 29: 5542-5556, <strong>IEEE TIP</strong>, 2020. <a href="https://ieeexplore.ieee.org/abstract/document/9058976" target="_self">PDF</a> </p></li>
<li><p>Kai Niu, <strong>Yan Huang</strong>, and Liang Wang, Re-ranking Image-text Matching by Adaptive Metric Fusion, <strong>PR</strong>, accepted, 2020. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320320301540" target="_self">PDF</a> </p></li>
  
  
<li><p>Chunfeng Song, Yongzhen Huang, <strong>Yan Huang</strong>, Ning Jia, and Liang Wang, GaitNet: An End-to-end Network for Gait Based Human Identification, <strong>PR</strong>, accepted, 2019. <a href="https://www.sciencedirect.com/science/article/pii/S0031320319302912" target="_self">PDF</a> </p></li>
<li><p>Linjiang Huang, <strong>Yan Huang</strong>, Wanli Ouyang, and Liang Wang, Part-Aligned Pose-Guided Recurrent Network for Action Recognition, <strong>PR</strong>, accepted, 2019. <a href="https://www.sciencedirect.com/science/article/pii/S0031320319301098" target="_self">PDF</a> </p></li>
<li><p>Yanyun Wang, Chunfeng Song, <strong>Yan Huang</strong>, and Liang Wang, Learning View Invariant Gait Features with Two-Stream GAN, <strong>Neurocomputing</strong>, accepted, 2019. <a href="https://www.sciencedirect.com/science/article/pii/S0925231219302395" target="_self">PDF</a> </p></li>
<li><p>Qiang Cui, Shu Wu, <strong>Yan Huang</strong>, and Liang Wang, A Hierarchical Contextual Attention-based Network for Sequential Recommendation, <strong>Neurocomputing</strong>, accepted, 2019. <a href="https://www.sciencedirect.com/science/article/pii/S0925231219306642" target="_self">PDF</a> </p></li>
<li><p><strong>Yan Huang</strong>, Qi Wu, Wei Wang, and Liang Wang, Image and Sentence Matching via Semantic Concepts and Order Learning, <strong>IEEE TPAMI</strong>, accepted, 2019. <a href="https://ieeexplore.ieee.org/document/8550752" target="_self">PDF</a> </p></li>
<li><p><strong>Yan Huang</strong>, Wei Wang, and Liang Wang, Video Super-resolution via Bidirectional Recurrent Convolutional Networks, <strong>IEEE TPAMI</strong>, 40(4), 1015-1028, 2018. <a href="https://ieeexplore.ieee.org/abstract/document/7919264" target="_self">PDF</a> </p></li>
<li><p><strong>Yan Huang</strong>, Wei Wang, Liang Wang, and Tieniu Tan, Conditional High-order Boltzmann Machines for Supervised Relation Learning, <strong>IEEE TIP</strong>, 26(9):4297-4310, 2017. <a href="https://ieeexplore.ieee.org/document/7913581" target="_self">PDF</a> </p></li>
<li><p><strong>Yan Huang</strong>, Wei Wang, and Liang Wang, Unconstrained Multimodal Multi-Label Learning, <strong>IEEE TMM</strong>, 17(11):1923-1935, 2015. <a href="https://ieeexplore.ieee.org/document/7239600" target="_self">PDF</a> </p></li>
</ul>

<h2>Conference Papers</h2> 
<ul>
<li><p>Linjiang Huang, <strong>Yan Huang</strong>, Wanli Ouyang, and Liang Wang, Relational Prototypical Network for Weakly Supervised Temporal Action Localization, <strong>AAAI</strong>, accepted, 2020. (<font color="#FF0000">Oral</font>) <a href="" target="_self">PDF</a> </p></li>
<li><p>Linjiang Huang, <strong>Yan Huang</strong>, Wanli Ouyang, and Liang Wang, Part-Level Graph Convolutional Network for Skeleton-Based Action Recognition, <strong>AAAI</strong>, accepted, 2020. (<font color="#FF0000">Oral</font>) <a href="" target="_self">PDF</a> </p></li>


<li><p><strong>Yan Huang</strong> and Liang Wang, ACMM: Aligned Cross-Modal Memory For Few-Shot Image and Sentence Matching, <strong>ICCV</strong>, accepted, 2019. <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_ACMM_Aligned_Cross-Modal_Memory_for_Few-Shot_Image_and_Sentence_Matching_ICCV_2019_paper.pdf" target="_self">PDF</a> </p></li>
<li><p><strong>Yan Huang</strong>, Yang Long, and Liang Wang, Few-Shot Image and Sentence Matching via Gated Visual-Semantic Embedding, <strong>AAAI</strong>, accepted, 2019. (<font color="#FF0000">Spotlight</font>) <a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4866/4739" target="_self">PDF</a> </p></li>
<li><p>Weining Wang, <strong>Yan Huang</strong>, and Liang Wang, Language-driven Temporal Activity Localization: A Semantic Matching Reinforcement Learning Model, <strong>CVPR</strong>, accepted, 2019. (<font color="#FF0000">Oral</font>) <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Language-Driven_Temporal_Activity_Localization_A_Semantic_Matching_Reinforcement_Learning_Model_CVPR_2019_paper.pdf" target="_self">PDF</a> </p></li>
<li><p>Chunfeng Song, <strong>Yan Huang</strong>, Wanli Ouyang, and Liang Wang, Box-driven Class-wise Region Masking and Filling Rate Guided Loss for Weakly Supervised Semantic Segmentation, <strong>CVPR</strong>, accepted, 2019. <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Song_Box-Driven_Class-Wise_Region_Masking_and_Filling_Rate_Guided_Loss_for_CVPR_2019_paper.pdf" target="_self">PDF</a> </p></li>
<li><p>Kai Niu, <strong>Yan Huang</strong>, and Liang Wang, Fusing Two Directions in Cross-domain Adaption for Real Life Person Search by Language, <strong>ICCV Workshop</strong>, 2019. (<font color="#FF0000">Oral</font>) <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/WIDER/Niu_Fusing_Two_Directions_in_Cross-Domain_Adaption_for_Real_Life_Person_ICCVW_2019_paper.pdf" target="_self">PDF</a> </p></li>
<li><p>Zerui Chen, <strong>Yan Huang</strong>, and Liang Wang, Learning Depth-aware Heatmaps for 3D Human Pose Estimation in the Wild, <strong>BMVC</strong>, 2019. <a href="https://bmvc2019.org/wp-content/uploads/papers/1217-paper.pdf" target="_self">PDF</a> </p></li>
<li>Wu Zheng, Lin Li, Zhaoxiang Zhang, <strong>Yan Huang</strong>, and Liang Wang, Relational Network for Skeleton-Based Action Recognition, <strong>ICME</strong>, 2019. (<font color="#FF0000">Oral</font>) <a href="https://ieeexplore_ieee.xilesou.top/abstract/document/8784961" target="_self">PDF</a> </p></li>
<li><p>Hongyuan Yu, <strong>Yan Huang</strong>, Lihong Pi, and Liang Wang, Recurrent Deconvolutional Generative Adversarial Networks with Application to Video Generation, <strong>PRCV</strong>, 2019. <a href="" target="_self">PDF</a> </p></li>
<li><p>Linjiang Huang, <strong>Yan Huang</strong>, Wanli Ouyang, and Liang Wang, Hierarchical Graph Convolutional Network For Skeleton-Based Action Recognition, <strong>ICIG</strong>, accepted, 2019. <a href="" target="_self">PDF</a> </p></li>
<li><p>Zerui Chen, <strong>Yan Huang</strong>, and Liang Wang, Augmented Visual-Semantic Embeddings for Image and Sentence Matching, <strong>ICIP</strong>, accepted, 2019. <a href="https://ieeexplore_ieee.xilesou.top/abstract/document/8802975" target="_self">PDF</a> </p></li>



<li><p><strong>Yan Huang</strong>, Qi Wu, Chunfeng Song, and Liang Wang, Learning Semantic Concepts and Order for Image and Sentence Matching, <strong>CVPR</strong>, 6163-6171, 2018. (<font color="#FF0000">Spotlight</font>) <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_Semantic_Concepts_CVPR_2018_paper.pdf" target="_self">PDF</a> </p></li>
<li><p>Chunfeng Song, <strong>Yan Huang</strong>, Wanli Ouyang, and LiangWang, Mask-Guided Contrastive Attention Model for Person Re-Identification, <strong>CVPR</strong>, pp. 1179-1188, 2018. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.pdf" target="_self">PDF</a> </p></li>
<li><p>Junbo Wang, Wei Wang, <strong>Yan Huang</strong>, Liang Wang, and Tieniu Tan, Multimodal Memory Modelling for Video Captioning, <strong>CVPR</strong>, pp. 7512-7520, 2018. (<font color="#FF0000">Spotlight</font>) <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_M3_Multimodal_Memory_CVPR_2018_paper.pdf" target="_self">PDF</a> </p></li>
<li><p>Junbo Wang, Wei Wang, <strong>Yan Huang</strong>, Liang Wang, and Tieniu Tan, Hierarchical Memory Modelling for Video Captioning, <strong>ACM Multimedia</strong>, 2018, accepted. <a href="http://delivery.acm.org/10.1145/3250000/3240538/p63-wang.pdf?ip=159.226.178.147&id=3240538&acc=ACTIVE%20SERVICE&key=33E289E220520BFB%2E949A0B1AADF887FF%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1551078295_e37305fd2da577784915927485e580e7" target="_self">PDF</a> </p></li>
<li><p>Chenglong Li, Chengli Zhu, <strong>Yan Huang</strong>, Jin Tang, and Liang Wang, Cross-Modal Ranking with Soft Consistency and Noisy Labels for Robust RGB-T Tracking, <strong>ECCV</strong>, 2018, accepted. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Chenglong_Li_Cross-Modal_Ranking_with_ECCV_2018_paper.pdf" target="_self">PDF</a> </p></li>
<li><p>Wenlong Cheng, <strong>Yan Huang</strong>, and Liang Wang, Towards Unconstrained Pointing Problem of Visual Question Answering: A Retrieval-Based Method, <strong>ICPR</strong>, 2018, accepted. (<font color="#FF0000">Oral</font>) <a href="https://ieeexplore.ieee.org/document/8546015" target="_self">PDF</a> </p></li>
<li><p>Lin Li, Zhaoxiang Zhang, <strong>Yan Huang</strong>, and Liang Wang, Deep Temporal Feature Encoding for Action Recognition, <strong>ICPR</strong>, 2018, accepted. (<font color="#FF0000">Oral</font>) <a href="https://ieeexplore.ieee.org/document/8546263" target="_self">PDF</a> </p> </li>

  

<li><p><strong>Yan Huang</strong>, Wei Wang, and Liang Wang, Instance-aware Image and Sentence Matching with Selective Multimodal LSTM, <strong>CVPR</strong>, pp. 2310-2318, 2017. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Instance-Aware_Image_and_CVPR_2017_paper.pdf" target="_self">PDF</a> </p></li>
<li><p>Zhen Zhou, <strong>Yan Huang</strong>, Wei Wang, Liang Wang, and Tieniu Tan, See the forest for the trees: Joint spatial and temporal recurrent neural networks for video-based person re-identification, <strong>CVPR</strong>, pp. 6776-6785, 2017. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_See_the_Forest_CVPR_2017_paper.pdf" target="_self">PDF</a> </p></li>
<li><p>Qiyue Yin, <strong>Yan Huang</strong>, and Liang Wang, Learning Shared and Specific Factors for Multi-modal Data, <strong>CCCV</strong>, pp. 89-98, 2017. <a href="https://link.springer.com/chapter/10.1007/978-981-10-7302-1_8" target="_self">PDF</a> </p></li>

  
<li><p><strong>Yan Huang</strong>, Wei Wang, and Liang Wang, Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution, <strong>NIPS</strong>, pp. 235-243, 2015. <a href="https://papers.nips.cc/paper/5778-bidirectional-recurrent-convolutional-networks-for-multi-frame-super-resolution.pdf" target="_self">PDF</a> </p></li>
<li><p><strong>Yan Huang</strong>, Wei Wang, and Liang Wang, Conditional High-order Boltzmann Machine: A Supervised Learning Model for Relation Learning, <strong>ICCV</strong>, pp. 4265-4273, 2015. <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Huang_Conditional_High-Order_Boltzmann_ICCV_2015_paper.pdf" target="_self">PDF</a> </p></li>


<li><p><strong>Yan Huang</strong>, Wei Wang, Liang Wang, and Tieniu Tan, A General Nonlinear Embedding Framework Based on Deep Neural Network, <strong>ICPR</strong>, pp. 732-737, 2014. (<font color="#FF0000">Oral</font>) <a href="https://ieeexplore.ieee.org/document/6976846" target="_self">PDF</a> </p></li>
<li><p>Peihao Huang, <strong>Yan Huang</strong>, Wei Wang, and Liang Wang, Deep Embedding Network for Clustering, <strong>ICPR</strong>, pp. 1532-1537, 2014. (<font color="#FF0000">Best Student Paper Award</font>) <a href="https://ieeexplore.ieee.org/document/6976982" target="_self">PDF</a> </p></li>
<li><p>Wei Wang, <strong>Yan Huang</strong>, Yizhou Wang, and Liang Wang, Generalized Autoencoder: A Neural Network Framework for Dimensionality Reduction, <strong>CVPR Workshop</strong> , pp. 490-497, 2014. (<font color="#FF0000">Best Paper Award</font>) <a href="http://openaccess.thecvf.com/content_cvpr_workshops_2014/W15/papers/Wang_Generalized_Autoencoder_A_2014_CVPR_paper.pdf" target="_self">PDF</a> </p></li>
<li><p><strong>Yan Huang</strong>, Wei Wang, Liang Wang, and Tieniu Tan, Multi-task Deep Neural Network for Multi-label Learning, <strong>ICIP</strong>, pp. 2897-2900, 2013. (<font color="#FF0000">Oral</font>) <a href="https://ieeexplore.ieee.org/document/6738596" target="_self">PDF</a> </p></li>
<li><p><strong>Yan Huang</strong>, Wei Wang, Liang Wang, and Tieniu Tan, An Effective Regional Saliency Model Based on Extended Site Entropy Rate, <strong>ICPR</strong>, pp. 1407-1410, 2012. <a href="https://www.computer.org/csdl/proceedings/icpr/2012/2216/00/06460404.pdf" target="_self">PDF</a> </p></li>

</ul>


<h2>Competitions</h2> 
<ul>
<table class="imgtable"><tr><td>
<img src="/projects/2014/ILSVRC/intro.png" alt="alt text" width="120" height="80" /> &nbsp;</td>
<td align="left">
<p> ILSVRC 2014: ImageNet Large Scale Visual Recognition Challenge. Our team (Weiqiang Ren, Chong Wang, <b>Yanhua Cheng</b>, Kaiqi Huang, Tieniu Tan) is the winner of the task: Image classification with additional training data. See details here: <a href="http://image-net.org/challenges/LSVRC/2014/results">Results of ILSVRC2014</a>.
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="/projects/2015/SHREC/intro.png" alt="alt text" width="120" height="60" /> &nbsp;</td>
<td align="left">
<p>SHREC’15 Track: 3D Object Retrieval with Multimodal Views Challenge. Our team (<b>Yanhua Cheng</b>, Xin Zhao, Kaiqi Huang, Tieniu Tan) win the first place on NN and NDCG criterion for the task "retrieval based on 721 images of each object". See details here: <a href="/projects/2015/SHREC/paper.pdf">Results of SHREC'15 Track</a>.
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="/projects/2015/alibaba/intro.png" alt="alt text" width="120" height="70" /> &nbsp;</td>
<td align="left">
<p>阿里巴巴大规模图像搜索大赛2015. 我们小组(<b>Yanhua Cheng</b>, Yueying Kao, Dangwei Li)</a> 在843只队伍脱颖而出晋级决赛并获得了优秀奖. 具体详见: <a href="https://tianchi.aliyun.com/competition/introduction.htm?spm=5176.100067.5678.1.HuBh0Y&raceId=231510">天池竞赛</a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="/projects/2010/FIFA/intro.png" alt="alt text" width="120" height="80" /> &nbsp;</td>
<td align="left">
<p>第十届全国机器人大赛暨2010年FIRA世界杯机器人大赛中国队选拔赛. 我们代表华中科技大学获得了类人型机器人障碍跑冠军和类人型机器人举重亚军. 具体详见: <a href="http://v.youku.com/v_show/id_XMjcxNzI3MzI4.html">youku</a>; <a href="/projects/2010/FIFA/pic.jpg">pic</a>
</p>
</td></tr></table>

</ul>

<div id="footer">
<div id="footer-text">
</br>Last updated at 2016-04-08 by Yanhua Cheng.
</div>
</div>

</div>
</body>
</html>
